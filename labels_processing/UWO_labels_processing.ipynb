{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically import the libraries into Jupyter Notebook\n",
    "%run ../functions/startup.py\n",
    "from _funct_annotat import Labels_proc\n",
    "lab_p = Labels_proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_dir(directory):\n",
    "    import os\n",
    "    return os.path.dirname(directory)\n",
    "\n",
    "current_dirs_parent = get_parent_dir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = str(current_dirs_parent)\n",
    "folder = '/data_examples/UWO_data/'  # Select data folder\n",
    "\n",
    "path_all = path_base + folder\n",
    "save_path = path_all     # Destination folder to for labelled data\n",
    "name_of_file1 = 's1.csv'\n",
    "\n",
    "name_of_file_l1_time = \"Labels_Anomaly_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD zeros and ones with dictionary mapping\n",
    "\n",
    "mapper_dict = {'left_only': 0, 'both': 1}\n",
    "def mp(entry):\n",
    "    \"\"\"\n",
    "    maps new values\n",
    "    \"\"\"\n",
    "    return mapper_dict[entry] if entry in mapper_dict else entry\n",
    "mp = np.vectorize(mp)\n",
    "\n",
    "\n",
    "def comp_labs_case1 (data_lab_1, data1_l):\n",
    "    labels_df_1 = pd.merge(data_lab_1,data1_l, on = ['date', 'time'], how='left', indicator=True)\n",
    "    labels_df_1 ['_merge'] = mp(labels_df_1['_merge'])\n",
    "    labels_df_1 = labels_df_1.rename(index=str, columns={\"_merge\": \"Anomaly\"})\n",
    "    \n",
    "    data_1 = labels_df_1.copy()\n",
    "    data_1['datetime'] = pd.to_datetime(labels_df_1['date']+ ' ' + labels_df_1['time'])\n",
    "    data_1.drop_duplicates('datetime',inplace=True)\n",
    "    if data_lab_1.shape[1] == 5:\n",
    "        print('Fraction of collective anomalies bf_03, bf_04:', np.sum(data_1['Anomaly'] ==1)/data_1.shape[0])\n",
    "    else:\n",
    "        print('Fraction of anomalies in ', data_lab_1.keys()[2], ': ', np.sum(data_1['Anomaly'] ==1)/data_1.shape[0])\n",
    "    print('Shape:', data_1.shape)\n",
    "    data_1.reset_index(inplace=True)\n",
    "    \n",
    "    return data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, explore and manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completePath = os.path.join(path_all, name_of_file1) \n",
    "df = pd.read_csv(completePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy(deep=True)\n",
    "df2['date'], df2['time'] = df2['time'].str.split(' ', 1).str\n",
    "\n",
    "sr0 = df2.keys()[1]\n",
    "sr1 = df2.keys()[2]\n",
    "sr2 = df2.keys()[3]\n",
    "print('Sensor names:',sr0,',', sr1,',', sr2)\n",
    "\n",
    "# Create datetime\n",
    "\n",
    "df2['date'] = [x.date() for x in (pd.to_datetime([i for i in df2['date']], format='%Y-%m-%d'))] \n",
    "df2['time'] = [x.time() for x in (pd.to_datetime([i for i in df2['time']], format='%H:%M:%S'))]   # remove primes from the time\n",
    "df1 = df2.copy(deep=True)\n",
    "df2.set_index(['date','time'], inplace=True)\n",
    "\n",
    "df_bf_00 = df2[sr0]\n",
    "df_bf_01 = df2[sr1]\n",
    "df_bf_02 = df2[sr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dates where temporal resolution is wrong\n",
    "#df2, date_list = lab_p.rem_dates(df2, df_bf_00, drop = False)\n",
    "\n",
    "# Get date_time\n",
    "data_df = df2.copy()\n",
    "data_lab_1 = lab_p.get_date_time(data_df, sr0)\n",
    "data_lab_2 = lab_p.get_date_time(data_df, sr1)\n",
    "data_lab_3 = lab_p.get_date_time(data_df, sr2)\n",
    "data_lab_123 = lab_p.get_date_time(data_df, sr0, multi= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and compute labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_l = pd.read_csv(os.path.join(path_all, name_of_file_l1_time +sr0 + '.csv'))  \n",
    "data2_l = pd.read_csv(os.path.join(path_all, name_of_file_l1_time +sr1 + '.csv'))  \n",
    "data3_l = pd.read_csv(os.path.join(path_all, name_of_file_l1_time +sr2 + '.csv')) \n",
    "data123_l = pd.read_csv(os.path.join(path_all, name_of_file_l1_time +sr0 + sr1 +sr2 + '.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_l.drop(['Unnamed: 0', data1_l.keys()[3],'Anomaly' ], axis=1, inplace=True)\n",
    "data2_l.drop(['Unnamed: 0',data2_l.keys()[3],'Anomaly' ], axis=1, inplace=True)\n",
    "data3_l.drop(['Unnamed: 0',data3_l.keys()[3],'Anomaly' ], axis=1, inplace=True)\n",
    "data123_l.drop(['Unnamed: 0',data123_l.keys()[3],data123_l.keys()[4],data123_l.keys()[5],'Anomaly' ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = comp_labs_case1 (data_lab_1, data1_l)\n",
    "data_2 = comp_labs_case1 (data_lab_2, data2_l)\n",
    "data_3 = comp_labs_case1 (data_lab_3, data3_l)\n",
    "data_123 = comp_labs_case1 (data_lab_123, data123_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge anomalies\n",
    "\n",
    "TT = data_1.Anomaly + data_2.Anomaly + data_123.Anomaly\n",
    "for i, x in enumerate(TT):\n",
    "    if x >= 1: TT[i] = 1\n",
    "        \n",
    "list_df = list(zip(data_123.datetime, data_123.bf_03, data_123.bf_04, data_123.bl_ce193, TT))\n",
    "merg_data_df = pd.DataFrame(list_df, columns=['datetime', 'bf_03', 'bf_04', 'bl_ce193','Tot_anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name = 'UWO_Anomaly_collective' \n",
    "lab_p.save_pkl(name,  path_base, folder, merg_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = lab_p.rearr_cols(data_1)\n",
    "data_2 = lab_p.rearr_cols(data_2)\n",
    "data_3 = lab_p.rearr_cols(data_3)\n",
    "data_123 = lab_p.rearr_cols(data_123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name1 = 'bf_03_Anomaly' \n",
    "name2 = 'bf_04_Anomaly' \n",
    "name3 = 'bl_ce193_Anomaly' \n",
    "name4 = 'bf_03_bf_04_bl_ce193_Anomaly' \n",
    "\n",
    "lab_p.save_pkl(name1,  path_base, folder, data_1)\n",
    "lab_p.save_pkl(name2, path_base, folder, data_2)\n",
    "lab_p.save_pkl(name3,  path_base, folder, data_3)\n",
    "lab_p.save_pkl(name4,  path_base, folder, data_123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADAS_env",
   "language": "python",
   "name": "adas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
