{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../functions/startup.py\n",
    "random.seed(4)\n",
    "\n",
    "from _funct_annotat import Labels_proc_NEST\n",
    "lab_p = Labels_proc_NEST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_dir(directory):\n",
    "    import os\n",
    "    return os.path.dirname(directory)\n",
    "\n",
    "current_dirs_parent = get_parent_dir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = str(current_dirs_parent)\n",
    "folder = '/data_examples/NEST_data/'  # Select data folder\n",
    "labels = 'labels/'\n",
    "\n",
    "path_all = path_base + folder\n",
    "save_path = path_all     # Destination folder to for labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge single anomalies to collective anomalies for 2 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Case\n",
    "Case = 1      # case 1: GAK (p3,p4),       case 2: Pressure T1 T2 (p1,p2)\n",
    "\n",
    "if Case == 1:\n",
    "    lab1 = '_labels_timep3'\n",
    "    lab2 = '_labels_timep4'\n",
    "    lab3 = '_labels_timep3p4'\n",
    "if Case == 2:\n",
    "    lab1 = '_labels_timep1'\n",
    "    lab2 = '_labels_timep2'\n",
    "    lab3 = '_labels_timep1p2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Load multiple sensors anomalies\n",
    "\n",
    "data_all_labels = lab_p.data_labels_load(path_base,folder, labels, lab3)\n",
    "data_all_labels = lab_p.process_data(data_all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Load single sensors anomalies\n",
    "\n",
    "data_1 = lab_p.data_labels_load(path_base,folder, labels, lab1)\n",
    "data_1 = lab_p.process_data(data_1)\n",
    "\n",
    "data_2 = lab_p.data_labels_load(path_base,folder, labels, lab2)\n",
    "data_2 = lab_p.process_data(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if dates coincide and how many anomalies are different\n",
    "\n",
    "print(data_1.shape[0] == (np.sum(data_1.datetime == data_2.datetime)))\n",
    "print('Missing anomalies', data_all_labels.shape[0] - np.sum(data_1.Anomaly == data_all_labels.Anomaly))\n",
    "print('Missing anomalies', data_all_labels.shape[0] - np.sum(data_2.Anomaly == data_all_labels.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anom_all = data_all_labels.Anomaly.copy()\n",
    "count = 0\n",
    "for i in range (data_1.shape[0]):\n",
    "    if ((data_1.datetime[i] == data_all_labels.datetime[i]) & (data_1.Anomaly[i] == 1)):\n",
    "        count = count +1\n",
    "        Anom_all[i] = 1\n",
    "print('Total added anomalies:', count)\n",
    "\n",
    "# we do not initialize again Amoal_all\n",
    "count = 0\n",
    "for i in range (data_2.shape[0]):\n",
    "    if ((data_2.datetime[i] == data_all_labels.datetime[i]) & (data_2.Anomaly[i] == 1)):\n",
    "        count = count +1\n",
    "        Anom_all[i] = 1\n",
    "print('Total added anomalies:', count)\n",
    "\n",
    "print('Fraction of combined anomalies: ', np.sum(Anom_all)/data_all_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_all_labels['Tot_anomalies'] = Anom_all\n",
    "data_all_labels.drop(columns = ['index', 'datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "if Case == 1:\n",
    "    name = 'GAK_Anomaly_collective' \n",
    "else:\n",
    "    name = 'T1_T2_Anomaly_collective' \n",
    "\n",
    "file = open(path_all + labels + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(data_all_labels, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: merge the 2 collective cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = 'GAK_Anomaly_collective'\n",
    "name2 = 'T1_T2_Anomaly_collective'\n",
    "\n",
    "\n",
    "# Load data file  - Working with collective anomalies\n",
    "    \n",
    "file = open(path_all + labels + name1, 'rb')  # open a file, where you stored the pickled data\n",
    "df1 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n",
    "\n",
    "\n",
    "file = open(path_all + labels + name2, 'rb')  # open a file, where you stored the pickled data\n",
    "df2 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = lab_p.process_data_coll(df1)\n",
    "df2 = lab_p.process_data_coll(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing anomalies', df1.Tot_anomalies.shape[0] - np.sum(df1.Tot_anomalies == df2.Tot_anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = df1.Tot_anomalies + df2.Tot_anomalies\n",
    "\n",
    "for i, x in enumerate(TT):\n",
    "    if x >= 1: TT[i] = 1\n",
    "        \n",
    "list_df = list(zip(df1.day, df1.time, df2.p1, df2.p2, df1.p3, df1.p4, TT))\n",
    "\n",
    "merg_data_df = pd.DataFrame(list_df, columns=['day', 'time', 'p1', 'p2', 'p3', 'p4', 'Tot_anomalies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of anomalies: ', np.sum(merg_data_df['Tot_anomalies'] ==1)/merg_data_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name = 'NEST_Anomaly_collective' \n",
    "\n",
    "file = open(path_all + labels + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(merg_data_df, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation - SUBSAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file  - Working with collective anomalies\n",
    "\n",
    "name = 'NEST_Anomaly_collective' \n",
    "    \n",
    "file = open(path_all + labels + name, 'rb')  # open a file, where you stored the pickled data\n",
    "df1 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n",
    "\n",
    "df1.fillna(0, inplace=True)\n",
    "print ('Data shape', df1.shape)\n",
    "print (df1.keys())\n",
    "\n",
    "print('Fraction of anomalies:',np.sum(df1.Tot_anomalies)/df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 6\n",
    "\n",
    "df_subs = df1.copy(deep=True)\n",
    "df_subs = df_subs.loc[::step]    # start:stop:step   , subsample every minute\n",
    "\n",
    "print('Fraction of anomalies:', np.sum(df_subs.Tot_anomalies)/df_subs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name = name + '_sub_' + str(step)\n",
    "\n",
    "file = open(path_all + labels + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(df_subs, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
