{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data sets:\n",
    "\n",
    "##### 1 Month (august) of labels missing in T1 and T2. It has been now removed from GAK\n",
    "##### 1 Month (may) of labels missing in GAK. It has been now removed from  T1 and T2\n",
    "\n",
    "- Case 1: GAK (p3,p4),       \n",
    "    - only p3 : Fraction of anomalies:  0.0570\n",
    "    - only p4 : Fraction of anomalies:  0.0470\n",
    "    - p3 and p4 : Fraction of anomalies 0.0239\n",
    "    \n",
    "    - Merged : 6.07%   \n",
    "   \n",
    "\n",
    "\n",
    "- Case 2: Pressure T1 T2 (p1,p2)\n",
    "    - only p1 : Fraction of anomalies:  0.04833761153893167  \n",
    "    - only p2 : Fraction of anomalies:  0.02308741290795746  \n",
    "    - p1 and p2 : Fraction of anomalies:  0.014922533920058673\n",
    "    \n",
    "    - Merged: 8.02%  ----- makes sense for Angelika as they all they influence\n",
    "   \n",
    "   \n",
    "   \n",
    " \n",
    "- Case 1 and Case 2 merged\n",
    "    - Fraction of anomalies:   13.35%\n",
    "    \n",
    "    \n",
    "# F1 results with RF model\n",
    "\n",
    "- note that full data cases present (almost) the results of the samesubsampled analogues and are not reported here \n",
    "\n",
    "## CASE 1 anomalies\n",
    "\n",
    "- p1 : 0.77\n",
    "- p2 : 0.75 \n",
    "- p1, p2: 0.91\n",
    "\n",
    "- merged: 0.81\n",
    "\n",
    "\n",
    "## CASE 2 anomalies\n",
    "\n",
    "- p3: 0.89\n",
    "- p4: 0.92\n",
    "- p3, p4: 0.74\n",
    "\n",
    "- merged: 0.88\n",
    "\n",
    "\n",
    "## MERGED CASES\n",
    "\n",
    "- 0.84 anomalies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries have been loaded.\n"
     ]
    }
   ],
   "source": [
    "# Automatically import the libraries into Jupyter Notebook\n",
    "%run functions/startup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On personal laptop\n",
    "path_base = ('/Users/russoste/Desktop/my_git_repos/00_Data/04_Nest/')\n",
    "path_figures = ('/Users/russoste/Desktop/my_git_repos/00_Data/04_Nest/figures/')\n",
    "path_data_saved = ('/Users/russoste/Desktop/my_git_repos/00_Data/04_Nest/Sup_Unsup_data_saved/')\n",
    "path_data_raw = 'data_raw/'\n",
    "path_data_processed = 'data_processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Case\n",
    "Case = 1      # case 1: GAK (p3,p4),       case 2: Pressure T1 T2 (p1,p2)\n",
    "\n",
    "if Case == 1:\n",
    "    folder = path_data_raw + 'data_pressure_sensor_GAK/'\n",
    "    labels = 'labels/'\n",
    "    lab1 = '_labels_timep3'\n",
    "    lab2 = '_labels_timep4'\n",
    "    lab3 = '_labels_timep3p4'\n",
    "if Case == 2:\n",
    "    folder = path_data_raw + 'data_pressure_sensors_T1_T2/' \n",
    "    labels = 'labels/'\n",
    "    lab1 = '_labels_timep1'\n",
    "    lab2 = '_labels_timep2'\n",
    "    lab3 = '_labels_timep1p2'\n",
    "\n",
    "\n",
    "#Select sensor to work on\n",
    "ss = lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labels_load (path_all, folder, labels, ss):\n",
    "    ss = ss + '.csv'\n",
    "    data_files = []\n",
    "    data_files_path = []\n",
    "    for name in os.listdir(os.path.join(path_all,folder, labels)):\n",
    "    #if os.path.isdir(os.path.join(path_all, name)):\n",
    "        if name.endswith(ss):\n",
    "            data_files.append(name)\n",
    "            data_files_path.append(os.path.join(path_all,folder, labels, name))\n",
    "        data_files_path = sorted(data_files_path) \n",
    "        \n",
    "    tmp = []\n",
    "    data_all = pd.DataFrame()\n",
    "    for fn in data_files_path:\n",
    "        df = pd.read_csv(fn,index_col=[0])\n",
    "        #data_all.append(df)\n",
    "        tmp = pd.concat([data_all, df] , ignore_index=True)\n",
    "        data_all = tmp.copy()\n",
    "        \n",
    "    return data_all\n",
    "\n",
    "\n",
    "def process_data(data_all_labels):\n",
    "    data_all_labels['datetime'] = pd.to_datetime(data_all_labels['day']+ ' ' + data_all_labels['time'])\n",
    "    data_all_labels.drop_duplicates('datetime',inplace=True)\n",
    "    print('Fraction of anomalies: ', np.sum(data_all_labels['Anomaly'] ==1)/data_all_labels.shape[0])\n",
    "    print('Shape:', data_all_labels.shape)\n",
    "    data_all_labels.reset_index(inplace=True)\n",
    "    return data_all_labels\n",
    "\n",
    "def process_data_coll(data_all_labels):\n",
    "    data_all_labels['datetime'] = pd.to_datetime(data_all_labels['day']+ ' ' + data_all_labels['time'])\n",
    "    data_all_labels.drop_duplicates('datetime',inplace=True)\n",
    "    print('Fraction of anomalies: ', np.sum(data_all_labels['Tot_anomalies'] ==1)/data_all_labels.shape[0])\n",
    "    print('Shape:', data_all_labels.shape)\n",
    "    data_all_labels.reset_index(inplace=True)\n",
    "    return data_all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2619535, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_labels = data_labels_load(path_base,folder, labels, ss)\n",
    "data_all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a426257b8>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = data_all_labels.copy(deep=True)\n",
    "df2['datetime'] = pd.to_datetime(df2['day']+ ' ' + df2['time'])\n",
    "tmp_data = (df2['datetime'].diff()[1:]/np.timedelta64(10, 's')).values\n",
    "plt.plot(tmp_data[527036:527100])   # there is one month missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies:  0.023986318182425508\n"
     ]
    }
   ],
   "source": [
    "df4 = df2.copy()\n",
    "df4.drop_duplicates('datetime',inplace=True)\n",
    "print('Fraction of anomalies: ', np.sum(data_all_labels['Anomaly'] ==1)/data_all_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "if Case == 1:\n",
    "    name = 'GAK_Anomaly' + ss\n",
    "else:\n",
    "    name = 'T1_T2_Anomaly' + ss\n",
    "\n",
    "df4.drop(columns = 'datetime', inplace=True)\n",
    "file = open(path_base + path_data_processed + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(df4, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge single anomalies to collective anomalies for 2 sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Case\n",
    "Case = 1     # case 1: GAK (p3,p4),       case 2: Pressure T1 T2 (p1,p2)\n",
    "\n",
    "if Case == 1:\n",
    "    folder = path_data_raw + 'data_pressure_sensor_GAK/'\n",
    "    labels = 'labels/'\n",
    "    lab1 = '_labels_timep3'\n",
    "    lab2 = '_labels_timep4'\n",
    "    lab3 = '_labels_timep3p4'\n",
    "if Case == 2:\n",
    "    folder = path_data_raw + 'data_pressure_sensors_T1_T2/' \n",
    "    labels = 'labels/'\n",
    "    lab1 = '_labels_timep1'\n",
    "    lab2 = '_labels_timep2'\n",
    "    lab3 = '_labels_timep1p2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies:  0.023384213421342136\n",
      "Shape: (2617920, 6)\n"
     ]
    }
   ],
   "source": [
    "### Load multiple sensors anomalies\n",
    "\n",
    "data_all_labels = data_labels_load(path_base,folder, labels, lab3)\n",
    "data_all_labels = process_data(data_all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies:  0.055520413152426354\n",
      "Shape: (2617920, 5)\n",
      "Fraction of anomalies:  0.046593860774966384\n",
      "Shape: (2617920, 5)\n"
     ]
    }
   ],
   "source": [
    "### Load single sensors anomalies\n",
    "\n",
    "data_1 = data_labels_load(path_base,folder, labels, lab1)\n",
    "data_1 = process_data(data_1)\n",
    "\n",
    "data_2 = data_labels_load(path_base,folder, labels, lab2)\n",
    "data_2 = process_data(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Missing anomalies 105512\n",
      "Missing anomalies 104821\n"
     ]
    }
   ],
   "source": [
    "### check if dates coincide and how many anomalies are different\n",
    "\n",
    "print(data_1.shape[0] == (np.sum(data_1.datetime == data_2.datetime)))\n",
    "print('Missing anomalies', data_all_labels.shape[0] - np.sum(data_1.Anomaly == data_all_labels.Anomaly))\n",
    "print('Missing anomalies', data_all_labels.shape[0] - np.sum(data_2.Anomaly == data_all_labels.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total added anomalies: 145348\n"
     ]
    }
   ],
   "source": [
    "Anom_all = data_all_labels.Anomaly.copy()\n",
    "count = 0\n",
    "for i in range (data_1.shape[0]):\n",
    "    if ((data_1.datetime[i] == data_all_labels.datetime[i]) & (data_1.Anomaly[i] == 1)):\n",
    "        count = count +1\n",
    "        Anom_all[i] = 1\n",
    "print('Total added anomalies:', count)\n",
    "\n",
    "# we do not initialize again Amoal_all\n",
    "count = 0\n",
    "for i in range (data_2.shape[0]):\n",
    "    if ((data_2.datetime[i] == data_all_labels.datetime[i]) & (data_2.Anomaly[i] == 1)):\n",
    "        count = count +1\n",
    "        Anom_all[i] = 1\n",
    "print('Total added anomalies:', count)\n",
    "\n",
    "print('Fraction of combined anomalies: ', np.sum(Anom_all)/data_all_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>91.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>00:00:10</td>\n",
       "      <td>91.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-01 00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>00:00:20</td>\n",
       "      <td>91.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-01 00:00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>00:00:30</td>\n",
       "      <td>91.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-01 00:00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>00:00:40</td>\n",
       "      <td>91.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-01 00:00:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         day      time    p3    p4  Anomaly            datetime\n",
       "0      0  2018-06-01  00:00:00  91.6  12.6        0 2018-06-01 00:00:00\n",
       "1      1  2018-06-01  00:00:10  91.6  12.6        0 2018-06-01 00:00:10\n",
       "2      2  2018-06-01  00:00:20  91.6  12.6        0 2018-06-01 00:00:20\n",
       "3      3  2018-06-01  00:00:30  91.6  12.6        0 2018-06-01 00:00:30\n",
       "4      4  2018-06-01  00:00:40  91.6  12.6        0 2018-06-01 00:00:40"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_all_labels['Tot_anomalies'] = Anom_all\n",
    "data_all_labels.drop(columns = ['index', 'datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "if Case == 1:\n",
    "    name = 'GAK_Anomaly_collective' \n",
    "else:\n",
    "    name = 'T1_T2_Anomaly_collective' \n",
    "\n",
    "file = open(path_base + path_data_processed + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(data_all_labels, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the 2 collective cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = ('/Users/russoste/Desktop/my_git_repos/00_Data/04_Nest/')\n",
    "path_data_raw = 'data_raw/'\n",
    "path_data_processed = 'data_processed/'\n",
    "\n",
    "\n",
    "name1 = 'GAK_Anomaly_collective'\n",
    "name2 = 'T1_T2_Anomaly_collective'\n",
    "\n",
    "\n",
    "# Load data file  - Working with collective anomalies\n",
    "    \n",
    "file = open(path_base + path_data_processed + name1, 'rb')  # open a file, where you stored the pickled data\n",
    "df1 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n",
    "\n",
    "\n",
    "file = open(path_base + path_data_processed + name2, 'rb')  # open a file, where you stored the pickled data\n",
    "df2 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies:  0.06075013751375138\n",
      "Shape: (2617920, 7)\n",
      "Fraction of anomalies:  0.08253919141914191\n",
      "Shape: (2617920, 7)\n"
     ]
    }
   ],
   "source": [
    "df1 = process_data_coll(df1)\n",
    "df2 = process_data_coll(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing anomalies 323870\n"
     ]
    }
   ],
   "source": [
    "print('Missing anomalies', df1.Tot_anomalies.shape[0] - np.sum(df1.Tot_anomalies == df2.Tot_anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = df1.Tot_anomalies + df2.Tot_anomalies\n",
    "\n",
    "for i, x in enumerate(TT):\n",
    "    if x >= 1: TT[i] = 1\n",
    "        \n",
    "list_df = list(zip(df1.day, df1.time, df2.p1, df2.p2, df1.p3, df1.p4, TT))\n",
    "\n",
    "merg_data_df = pd.DataFrame(list_df, columns=['day', 'time', 'p1', 'p2', 'p3', 'p4', 'Tot_anomalies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies:  0.13350102371348246\n"
     ]
    }
   ],
   "source": [
    "print('Fraction of anomalies: ', np.sum(merg_data_df['Tot_anomalies'] ==1)/merg_data_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name = 'NEST_Anomaly_collective' \n",
    "\n",
    "file = open(path_base + path_data_processed + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(merg_data_df, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation - SUBSAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Case\n",
    "Case = 1     # case 1: GAK (p3,p4),       case 2: Pressure T1 T2 (p1,p2)\n",
    "\n",
    "if Case == 1:\n",
    "    name = 'GAK_Anomaly_collective'\n",
    "if Case == 2:\n",
    "    name = 'T1_T2_Anomaly_collective'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (2617920, 6)\n",
      "Index(['day', 'time', 'p3', 'p4', 'Anomaly', 'Tot_anomalies'], dtype='object')\n",
      "Fraction of anomalies: 0.06075013751375138\n"
     ]
    }
   ],
   "source": [
    "# Load data file  - Working with collective anomalies\n",
    "    \n",
    "file = open(path_base + path_data_processed + name, 'rb')  # open a file, where you stored the pickled data\n",
    "df1 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n",
    "\n",
    "df1.fillna(0, inplace=True)\n",
    "print ('Data shape', df1.shape)\n",
    "print (df1.keys())\n",
    "\n",
    "print('Fraction of anomalies:',np.sum(df1.Tot_anomalies)/df1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "###  Data manipulation 1\n",
    "- get half of the data\n",
    "**I can reduce the data to half without losing anomalies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies: 0.09499519084220233\n"
     ]
    }
   ],
   "source": [
    "half = df1.loc[0:df1.shape[0]/2]    \n",
    "print('Fraction of anomalies:', np.sum(half.Tot_anomalies)/half.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation 2\n",
    "\n",
    "- Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies: 0.060765034836817014\n"
     ]
    }
   ],
   "source": [
    "df_subs = df1.copy(deep=True)\n",
    "df_subs = df_subs.loc[::6]    # start:stop:step   , subsample every minute\n",
    "print('Fraction of anomalies:', np.sum(df_subs.Tot_anomalies)/df_subs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data Manipulation 2 to collective case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "if Case == 1:\n",
    "    name = 'GAK_Anomaly_collective_sub' \n",
    "else:\n",
    "    name = 'T1_T2_Anomaly_collective_sub' \n",
    "\n",
    "file = open(path_base + path_data_processed + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(df_subs, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Apply Data Manipulation 2 to all single sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'GAK_Anomaly_labels_timep3'\n",
    "# name = 'GAK_Anomaly_labels_timep4'\n",
    "name = 'GAK_Anomaly_labels_timep3p4'\n",
    "\n",
    "# name = 'T1_T2_Anomaly_labels_timep1'\n",
    "# name = 'T1_T2_Anomaly_labels_timep2'\n",
    "# name = 'T1_T2_Anomaly_labels_timep1p2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (2617920, 5)\n",
      "Index(['day', 'time', 'p3', 'p4', 'Anomaly'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load data file  - Working with collective anomalies\n",
    "    \n",
    "file = open(path_base + path_data_processed + name, 'rb')  # open a file, where you stored the pickled data\n",
    "df1 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n",
    "\n",
    "df1.fillna(0, inplace=True)\n",
    "print ('Data shape', df1.shape)\n",
    "print (df1.keys())\n",
    "\n",
    "print('Fraction of anomalies:',np.sum(df1.Anomaly)/df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs = df1.copy(deep=True)\n",
    "df_subs = df_subs.loc[::6]    # start:stop:step   , subsample every minute\n",
    "print('Fraction of anomalies:', np.sum(df_subs.Anomaly)/df_subs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name = name + '_sub'\n",
    "file = open(path_base + path_data_processed + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(df_subs, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data Manipulation 2 merged case1 and case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (2617920, 7)\n",
      "Index(['day', 'time', 'p1', 'p2', 'p3', 'p4', 'Tot_anomalies'], dtype='object')\n",
      "Fraction of anomalies: 0.13350102371348246\n"
     ]
    }
   ],
   "source": [
    "# Load data file  - Working with collective anomalies\n",
    "\n",
    "name = 'NEST_Anomaly_collective' \n",
    "    \n",
    "file = open(path_base + path_data_processed + name, 'rb')  # open a file, where you stored the pickled data\n",
    "df1 = pickle.load(file)   # dump information to that file\n",
    "file.close()   # close the file\n",
    "\n",
    "df1.fillna(0, inplace=True)\n",
    "print ('Data shape', df1.shape)\n",
    "print (df1.keys())\n",
    "\n",
    "print('Fraction of anomalies:',np.sum(df1.Tot_anomalies)/df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies: 0.1436210847975554\n"
     ]
    }
   ],
   "source": [
    "step = 6\n",
    "\n",
    "df_subs = df1.copy(deep=True)\n",
    "df_subs = df_subs.loc[::step]    # start:stop:step   , subsample every minute\n",
    "\n",
    "print('Fraction of anomalies:', np.sum(df_subs.Tot_anomalies)/df_subs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data as pickle\n",
    "\n",
    "name = name + '_sub_' + str(step)\n",
    "\n",
    "file = open(path_base + path_data_processed + name, 'wb')   # open a file, where you want to store the data\n",
    "pickle.dump(df_subs, file, protocol=4)   # dump information to that file\n",
    "file.close()   # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
